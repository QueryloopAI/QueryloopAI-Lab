import unsloth
import os
import re
import json
import random
import asyncio
import itertools
import numpy as np
import art
from art import Trajectory, TrajectoryGroup, TrainableModel, TrainConfig
from art.local import LocalBackend
from smolagents import tool, CodeAgent, LiteLLMModel
from rcwa import Material, Layer, LayerStack, Source, Solver
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
import pickle
from dataclasses import dataclass
from datetime import datetime
import glob
from pydantic import BaseModel

import logging
logging.basicConfig(
    filename="training.log",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger(__name__)

# Constants
start_wl = 0.32
stop_wl = 0.80
step_wl = 0.01
wavelengths = np.arange(start_wl, stop_wl + step_wl, step_wl)
materials = ['Si', 'Si3N4', 'SiO2', 'AlN']
os.environ["DISABLE_WEAVE"] = "1"

# ================================
# STEP 1: SMOLAGENTS TOOLS
# ================================

@tool
def simulate_spectrum(layer_order: List[str], thicknesses: List[float]) -> List[float]:
    """
    Simulates the optical transmission spectrum for a given material stack.
    Args:
        layer_order (List[str]): List of material names, e.g., ["Si", "AlN", "Si3N4", "SiO2"].
        thicknesses (List[float]): Thickness values (in nanometers) for each layer.
    Returns:
        List[float]: Transmission values across the wavelength range.
    """
    logger.info(f"üîß Called simulate_spectrum with order={layer_order}, thicknesses={thicknesses}")
    source = Source(wavelength=start_wl)
    reflection_layer = Layer(n=1.0)
    transmission_layer = Layer(material=Material("Si"))
    thicknesses_microns = [t * 0.001 for t in thicknesses]
    try:
        layers = [Layer(material=Material(m), thickness=t) for m, t in zip(layer_order, thicknesses_microns)]
        stack = LayerStack(*layers, incident_layer=reflection_layer, transmission_layer=transmission_layer)
        solver = Solver(stack, source, (1, 1))
        result = solver.solve(wavelength=wavelengths)
        return np.array(result['TTot']).tolist()
    except Exception as e:
        logger.error(f"Simulation failed: {e}")
        return []

@tool
def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """
    Calculates cosine similarity between two vectors.
    Args:
        vec1 (List[float]): First vector.
        vec2 (List[float]): Second vector.
    Returns:
        float: Cosine similarity value (-1 to 1).
    """
    a, b = np.array(vec1), np.array(vec2)
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

@tool
def mean_squared_error(vec1: List[float], vec2: List[float]) -> float:
    """
    Calculates mean squared error between two vectors.
    Args:
        vec1 (List[float]): Predicted values.
        vec2 (List[float]): Ground truth values.
    Returns:
        float: Mean squared error.
    """
    a, b = np.array(vec1), np.array(vec2)
    return float(np.mean((a - b) ** 2))

# ================================
# STEP 2: DATA STRUCTURES
# ================================

@dataclass
class TrainingExample:
    """A single training example with target spectrum and ground truth"""
    target_spectrum: List[float]
    true_order: List[str]
    true_thickness: float
    prompt: str
    answer: str

@dataclass
class GeneratedTrajectory:
    """A trajectory generated by smolagents"""
    example: TrainingExample
    messages: List[Dict[str, str]]
    final_response: str
    reward: float
    metadata: Dict[str, Any]

class OpticalSample(BaseModel):
    """Sample for optical RL training"""
    prompt: str
    ground_truth: str
    target_spectrum: List[float]
    true_order: List[str]
    true_thickness: float

# ================================
# STEP 3: DATASET CREATION
# ================================

def get_target_spectrum(order: List[str], thickness_nm: float) -> List[float]:
    """Generate target spectrum for given order and thickness"""
    return simulate_spectrum(order, [thickness_nm] * 4)

def create_training_examples(n_examples: int = 50) -> List[TrainingExample]:
    """Create training examples with diverse material orders and thicknesses"""
    examples = []
    perms = list(itertools.permutations(materials))
    thickness_options = [10, 100]
    all_combos = list(itertools.product(perms, thickness_options))
    
    for i in range(n_examples):
        order, thickness = random.choice(all_combos)
        order_list = list(order)
        target = get_target_spectrum(order_list, thickness)
        if not target:
            continue
        prompt = f"""You are a Nobel-winning optics AI tasked with finding the correct 4-layer material order and uniform thickness.

Available materials: Si, Si3N4, SiO2, AlN (use each exactly once)
Thickness options: 10nm or 100nm (must be uniform across all layers)

You have access to these tools:
- simulate_spectrum(order, thicknesses): Simulates optical spectrum
- cosine_similarity(predicted, target): Measures spectral similarity
- mean_squared_error(predicted, target): Measures spectral error

Your goal: Find the material order and thickness that produces this target spectrum.
Stop when cosine similarity > 0.999999.

Target Spectrum: {target}

Think systematically and use your tools efficiently. Always provide your final answer in this exact format:
Final Answer: Order: {order_list}, Thickness: {thickness}nm"""
        examples.append(TrainingExample(
            target_spectrum=target,
            true_order=order_list,
            true_thickness=thickness,
            prompt=prompt,
            answer=f"Order: {order_list}, Thickness: {thickness}nm"
        ))
    return examples

# ================================
# STEP 4: SMOLAGENTS DATA GENERATION
# ================================

def extract_order_from_response(text: str) -> List[str]:
    """Extract material order from agent response"""
    patterns = [
        r'Order:\s*\[([^\]]+)\]',
        r'order:\s*\[([^\]]+)\]',
        r'Final Answer:.*?Order:\s*\[([^\]]+)\]',
    ]
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            materials = [m.strip().strip("'\"") for m in match.group(1).split(',')]
            return materials
    return []

def extract_thickness_from_response(text: str) -> float:
    """Extract thickness from agent response"""
    patterns = [
        r'Thickness:\s*(\d+)nm',
        r'thickness:\s*(\d+)nm',
        r'Final Answer:.*?Thickness:\s*(\d+)nm',
    ]
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return float(match.group(1))
    return None

def compute_trajectory_reward(response: str, example: TrainingExample) -> float:
    """Compute reward for a generated trajectory"""
    reward = 0.0
    pred_order = extract_order_from_response(response)
    pred_thickness = extract_thickness_from_response(response)
    order_correct = pred_order == example.true_order
    thickness_correct = pred_thickness == example.true_thickness
    if order_correct and thickness_correct:
        reward += 2.0
    elif order_correct:
        reward += 1.0
    elif thickness_correct:
        reward += 0.5
    tool_calls = response.count("simulate_spectrum")
    efficiency_reward = max(0.0, 1.0 - (tool_calls - 1) / 20.0)
    reward += efficiency_reward
    if pred_order and pred_thickness:
        try:
            predicted_spectrum = simulate_spectrum(pred_order, [pred_thickness] * 4)
            if predicted_spectrum and len(predicted_spectrum) == len(example.target_spectrum):
                similarity = cosine_similarity(predicted_spectrum, example.target_spectrum)
                reward += max(0, similarity)
        except:
            pass
    return reward

class SmolagentsDataGenerator:
    """Generates training trajectories using smolagents"""
    
    def __init__(self, openai_api_key: str):
        self.model = LiteLLMModel(
            model_id="huggingface/together/mistralai/Mistral-Small-24B-Instruct-2501",
            api_key=openai_api_key
        )
        self.agent = CodeAgent(
            tools=[simulate_spectrum, cosine_similarity, mean_squared_error],
            model=self.model,
            stream_outputs=False,
            additional_authorized_imports=['numpy', 'itertools']
        )
    
    def generate_trajectory(self, example: TrainingExample) -> GeneratedTrajectory:
        """Generate a single trajectory for the given example"""
        try:
            result = self.agent.run(task=example.prompt)
            messages = [
                {"role": "system", "content": "You are a scientific AI agent specialized in optics."},
                {"role": "user", "content": example.prompt},
                {"role": "assistant", "content": result}
            ]
            reward = compute_trajectory_reward(result, example)
            trajectory = GeneratedTrajectory(
                example=example,
                messages=messages,
                final_response=result,
                reward=reward,
                metadata={
                    "timestamp": datetime.now().isoformat(),
                    "predicted_order": extract_order_from_response(result),
                    "predicted_thickness": extract_thickness_from_response(result),
                    "tool_calls": result.count("simulate_spectrum")
                }
            )
            logger.info(f"‚úÖ Generated trajectory - Reward: {reward:.4f}")
            return trajectory
        except Exception as e:
            logger.error(f"‚ùå Failed to generate trajectory: {e}")
            return GeneratedTrajectory(
                example=example,
                messages=[
                    {"role": "system", "content": "You are a scientific AI agent specialized in optics."},
                    {"role": "user", "content": example.prompt},
                    {"role": "assistant", "content": "I encountered an error during processing."}
                ],
                final_response="Error occurred",
                reward=-1.0,
                metadata={"error": str(e)}
            )

    def generate_dataset(self, examples: List[TrainingExample], 
                        trajectories_per_example: int = 1) -> List[GeneratedTrajectory]:
        """Generate multiple trajectories for each example"""
        all_trajectories = []
        for i, example in enumerate(examples):
            logger.info(f"üîÑ Generating trajectories for example {i+1}/{len(examples)}")
            for j in range(trajectories_per_example):
                trajectory = self.generate_trajectory(example)
                all_trajectories.append(trajectory)
        return all_trajectories

# ================================
# STEP 5: TRAINING DATA MANAGEMENT
# ================================

class OpticalRLTrainer:
    """Main trainer for smolagents data generation and ART training"""
    
    def __init__(self, openai_api_key: str, model_name: str = "Qwen/Qwen2.5-14B-Instruct"):
        self.openai_api_key = openai_api_key
        self.model_name = model_name
        self.data_generator = SmolagentsDataGenerator(openai_api_key)
        self.data_dir = Path("./optical_rl_data")
        self.data_dir.mkdir(exist_ok=True)
    
    def find_existing_data(self) -> Optional[str]:
        """Find the most recent generated trajectory file"""
        pattern = str(self.data_dir / "generated_trajectories_*.pkl")
        files = glob.glob(pattern)
        if not files:
            return None
        files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
        most_recent = files[0]
        logger.info(f"üìÅ Found existing data file: {most_recent}")
        return most_recent
    
    def check_data_quality(self, trajectories: List[GeneratedTrajectory]) -> Dict[str, Any]:
        """Check quality metrics of the loaded data"""
        if not trajectories:
            return {"valid": False, "reason": "No trajectories found"}
        total_trajectories = len(trajectories)
        successful_trajectories = sum(1 for t in trajectories if t.reward > 0)
        average_reward = sum(t.reward for t in trajectories) / total_trajectories
        high_reward_count = sum(1 for t in trajectories if t.reward > 1.5)
        rewards = [t.reward for t in trajectories]
        reward_variance = np.var(rewards) if len(rewards) > 1 else 0
        quality_metrics = {
            "valid": True,
            "total_trajectories": total_trajectories,
            "successful_trajectories": successful_trajectories,
            "success_rate": successful_trajectories / total_trajectories,
            "average_reward": average_reward,
            "high_reward_count": high_reward_count,
            "high_reward_rate": high_reward_count / total_trajectories,
            "reward_variance": reward_variance,
            "unique_rewards": len(set(rewards))
        }
        logger.info(f"üìä Data quality metrics: {quality_metrics}")
        return quality_metrics
    
    def add_reward_noise(self, trajectories: List[GeneratedTrajectory]) -> List[GeneratedTrajectory]:
        """Add small random noise to rewards"""
        for traj in trajectories:
            noise = random.uniform(-0.01, 0.01)
            traj.reward += noise
        logger.info("üé≤ Added reward noise")
        return trajectories
    
    def generate_training_data(self, n_examples: int = 20, 
                             trajectories_per_example: int = 2) -> List[GeneratedTrajectory]:
        """Generate training data using smolagents"""
        logger.info(f"üì¶ Generating {n_examples} examples with {trajectories_per_example} trajectories each")
        examples = create_training_examples(n_examples)
        trajectories = self.data_generator.generate_dataset(examples, trajectories_per_example)
        trajectories = self.add_reward_noise(trajectories)
        data_path = self.data_dir / f"generated_trajectories_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
        with open(data_path, 'wb') as f:
            pickle.dump(trajectories, f)
        logger.info(f"‚úÖ Generated {len(trajectories)} trajectories, saved to {data_path}")
        return trajectories
    
    def fix_missing_choices(self, trajectory: GeneratedTrajectory):
        """Patch missing choices in trajectory messages"""
        for msg in trajectory.messages:
            if msg["role"] == "assistant" and "choice Filled: linebreak choice" not in msg:
                msg["choice"] = {
                    "message": {"content": msg["content"]},
                    "logprobs": None,
                }
        return trajectory

    def load_training_data(self, data_path: str) -> List[GeneratedTrajectory]:
        """Load training data from file"""
        with open(data_path, 'rb') as f:
            trajectories = pickle.load(f)
        trajectories = [self.fix_missing_choices(t) for t in trajectories]
        logger.info(f"üì• Loaded {len(trajectories)} trajectories from {data_path}")
        return trajectories

    def get_or_generate_data(self, n_examples: int = 20, 
                           trajectories_per_example: int = 2, 
                           force_regenerate: bool = False) -> List[GeneratedTrajectory]:
        """Get existing data or generate new data"""
        if not force_regenerate:
            existing_data_path = self.find_existing_data()
            if existing_data_path:
                try:
                    trajectories = self.load_training_data(existing_data_path)
                    quality_metrics = self.check_data_quality(trajectories)
                    if (quality_metrics["valid"] and 
                        quality_metrics["success_rate"] > 0.3 and
                        quality_metrics["reward_variance"] > 0.01):
                        logger.info(f"üéØ Using existing data with {quality_metrics['success_rate']:.2%} success rate")
                        return trajectories
                    else:
                        logger.warning(f"‚ö†Ô∏è Existing data quality is poor, regenerating...")
                except Exception as e:
                    logger.error(f"‚ùå Failed to load existing data: {e}, regenerating...")
        logger.info("üîÑ Generating new training data...")
        return self.generate_training_data(n_examples, trajectories_per_example)

# ================================
# STEP 6: CONVERT TO ART TRAINING FORMAT
# ================================

def convert_generated_trajectory_to_optical_sample(trajectory: GeneratedTrajectory) -> OpticalSample:
    """Convert GeneratedTrajectory to OpticalSample"""
    return OpticalSample(
        prompt=trajectory.example.prompt,
        ground_truth=trajectory.example.answer,
        target_spectrum=trajectory.example.target_spectrum,
        true_order=trajectory.example.true_order,
        true_thickness=trajectory.example.true_thickness
    )

# ================================
# STEP 7: REWARD CALCULATION
# ================================

def calculate_reward_text_only(generated_response: str, sample: OpticalSample) -> float:
    """Fallback reward calculation"""
    pred_order = extract_order_from_response(generated_response)
    pred_thickness = extract_thickness_from_response(generated_response)
    reward = 0.0
    if pred_order == sample.true_order and pred_thickness == sample.true_thickness:
        reward = 1.0
    elif pred_order == sample.true_order:
        reward = 0.5
    elif pred_thickness == sample.true_thickness:
        reward = 0.2
    return reward

def execute_response_directly(generated_response: str, sample: OpticalSample) -> Dict[str, Any]:
    """Parse and execute tool calls directly with robust handling"""
    execution_results = {
        "tool_calls": 0,
        "simulations": [],
        "similarities": [],
        "errors": [],
        "final_order": None,
        "final_thickness": None,
        "success": False
    }
    
    try:
        # Parse simulate_spectrum calls with stricter pattern
        sim_pattern = r'simulate_spectrum\(\s*\[\s*([\'"][^\'"]+[\'"](?:\s*,\s*[\'"][^\'"]+[\'"]){3})\s*\]\s*,\s*\[\s*(\d+\.?\d*)\s*(?:,\s*\d+\.?\d*){3}\s*\]\s*\)'
        sim_matches = re.findall(sim_pattern, generated_response)
        
        for match in sim_matches:
            execution_results["tool_calls"] += 1
            try:
                # Parse order
                order = [m.strip().strip("'\"") for m in match[0].split(',')]
                # Validate order
                if len(order) != 4 or not all(m in materials for m in order) or len(set(order)) != 4:
                    execution_results["errors"].append(f"Invalid order: {order}")
                    continue
                
                # Parse thicknesses (assume uniform for simplicity)
                thickness = float(match[1])
                if thickness not in [10, 100]:
                    execution_results["errors"].append(f"Invalid thickness: {thickness}")
                    continue
                
                # Execute simulation
                spectrum = simulate_spectrum(order, [thickness] * 4)
                if spectrum and len(spectrum) == len(sample.target_spectrum):
                    execution_results["simulations"].append({
                        "order": order,
                        "thickness": thickness,
                        "spectrum": spectrum[:5]  # Store first 5 values for logging
                    })
                    similarity = cosine_similarity(spectrum, sample.target_spectrum)
                    execution_results["similarities"].append(similarity)
                else:
                    execution_results["errors"].append("Simulation returned empty or invalid spectrum")
            
            except Exception as e:
                execution_results["errors"].append(f"Tool execution failed: {str(e)}")
        
        # Extract final answer
        execution_results["final_order"] = extract_order_from_response(generated_response)
        execution_results["final_thickness"] = extract_thickness_from_response(generated_response)
        
        # Validate final answer
        if (execution_results["final_order"] and 
            len(execution_results["final_order"]) == 4 and 
            all(m in materials for m in execution_results["final_order"]) and 
            execution_results["final_thickness"] in [10, 100]):
            execution_results["success"] = True
        else:
            execution_results["errors"].append("Invalid final answer format")
    
    except Exception as e:
        execution_results["errors"].append(f"Overall execution failed: {str(e)}")
    
    logger.debug(f"Execution results: {execution_results}")
    return execution_results

def calculate_reward_from_direct_execution(execution_results: Dict[str, Any], 
                                         sample: OpticalSample) -> float:
    """Calculate reward from direct execution results"""
    total_reward = 0.0
    
    # Correctness rewards
    if execution_results["final_order"] == sample.true_order:
        total_reward += 1.0
    if execution_results["final_thickness"] == sample.true_thickness:
        total_reward += 1.0
    
    # Tool usage rewards
    if execution_results["tool_calls"] > 0:
        total_reward += 0.2
        efficiency = max(0.0, 0.3 - (execution_results["tool_calls"] - 1) * 0.05)
        total_reward += efficiency
    
    # Spectral similarity rewards
    if execution_results["similarities"]:
        max_similarity = max(execution_results["similarities"])
        total_reward += max_similarity * 0.5
    
    # Success bonus
    if execution_results["success"]:
        total_reward += 0.1
    
    # Error penalty
    if execution_results["errors"]:
        total_reward -= len(execution_results["errors"]) * 0.1
    
    return max(0.0, min(total_reward, 3.0))

# ================================
# STEP 8: ROLLOUT FUNCTIONS
# ================================

@art.retry()
async def rollout(model: art.Model, sample: OpticalSample) -> art.Trajectory:
    """Generate a trajectory using CodeAgent (original)"""
    trajectory = art.Trajectory(
        messages_and_choices=[
            {"role": "system", "content": "You are a scientific AI agent specialized in optics."},
            {"role": "user", "content": sample.prompt},
        ],
        reward=0,
    )
    messages = trajectory.messages()
    completion = await model.openai_client().chat.completions.create(
        messages=messages,
        model=model.name,
        max_completion_tokens=2048,
        temperature=0.3,
    )
    choice = completion.choices[0]
    generated_response = choice.message.content
    trajectory.messages_and_choices.append(choice)
    
    try:
        model_for_execution = LiteLLMModel(
            model_id="huggingface/together/mistralai/Mistral-Small-24B-Instruct-2501",
            api_key=""
        )
        execution_agent = CodeAgent(
            tools=[simulate_spectrum, cosine_similarity, mean_squared_error],
            model=model_for_execution,
            stream_outputs=False,
            additional_authorized_imports=['numpy', 'itertools']
        )
        execution_result = execution_agent.run(task=generated_response)
        reward = calculate_reward_from_execution(execution_result, sample, generated_response)
        logger.info(f"CodeAgent execution reward: {reward}")
        logger.debug(f"Generated: {generated_response[:200]}...")
        logger.debug(f"Execution result: {execution_result[:200]}...")
    except Exception as e:
        logger.error(f"Failed to execute generated response: {e}")
        reward = calculate_reward_text_only(generated_response, sample)
    
    trajectory.reward = reward
    return trajectory

def calculate_reward_from_execution(execution_result: str, sample: OpticalSample, 
                                  original_response: str) -> float:
    """Calculate reward based on CodeAgent execution"""
    total_reward = 0.0
    final_pred_order = extract_order_from_response(execution_result)
    final_pred_thickness = extract_thickness_from_response(execution_result)
    order_correct = final_pred_order == sample.true_order
    thickness_correct = final_pred_thickness == sample.true_thickness
    if order_correct and thickness_correct:
        total_reward += 2.0
    elif order_correct:
        total_reward += 1.0
    elif thickness_correct:
        total_reward += 0.3
    tool_calls = original_response.count("simulate_spectrum")
    cosine_calls = original_response.count("cosine_similarity")
    mse_calls = original_response.count("mean_squared_error")
    if tool_calls > 0:
        total_reward += 0.2
        efficiency_reward = max(0.0, 0.3 - (tool_calls - 1) * 0.05)
        total_reward += efficiency_reward
    if cosine_calls > 0 or mse_calls > 0:
        total_reward += 0.1
    if final_pred_order and final_pred_thickness:
        try:
            predicted_spectrum = simulate_spectrum(final_pred_order, [final_pred_thickness] * 4)
            if predicted_spectrum and len(predicted_spectrum) == len(sample.target_spectrum):
                similarity = cosine_similarity(predicted_spectrum, sample.target_spectrum)
                total_reward += max(0, similarity * 0.5)
        except Exception as e:
            logger.debug(f"Could not calculate spectral similarity: {e}")
    if "Error" not in execution_result and "error" not in execution_result.lower():
        total_reward += 0.1
    if "cosine similarity" in execution_result.lower() and "0.999" in execution_result:
        total_reward += 0.1
    return min(total_reward, 3.0)

@art.retry()
async def rollout_with_direct_execution(model: art.Model, sample: OpticalSample) -> art.Trajectory:
    """Robust rollout with direct tool execution"""
    trajectory = art.Trajectory(
        messages_and_choices=[
            {"role": "system", "content": "You are a scientific AI agent specialized in optics. Use simulate_spectrum, cosine_similarity, and mean_squared_error tools to find the correct material order and thickness."},
            {"role": "user", "content": sample.prompt},
        ],
        reward=0,
    )
    
    messages = trajectory.messages()
    try:
        completion = await model.openai_client().chat.completions.create(
            messages=messages,
            model=model.name,
            max_completion_tokens=2048,
            temperature=0.3,
        )
        choice = completion.choices[0]
        generated_response = choice.message.content
        trajectory.messages_and_choices.append(choice)
        
        # Direct execution of tool calls
        execution_results = execute_response_directly(generated_response, sample)
        reward = calculate_reward_from_direct_execution(execution_results, sample)
        
        trajectory.reward = reward
        logger.info(f"Direct execution reward: {reward}")
        logger.debug(f"Generated response: {generated_response[:200]}...")
        logger.debug(f"Execution results: {execution_results}")
    except Exception as e:
        logger.error(f"Rollout failed: {e}")
        reward = calculate_reward_text_only(generated_response, sample)
    
    trajectory.reward = reward
    return trajectory

# ================================
# STEP 8: ART TRAINING SETUP
# ================================

model = art.TrainableModel(
    name="optical-rl-agent",
    project="optical-rl-training",
    base_model="Qwen/Qwen2.5-14B-Instruct"
)

model._internal_config = art.dev.InternalModelConfig(
    init_args=art.dev.InitArgs(max_seq_length=16000),
    engine_args=art.dev.EngineArgs(
        enforce_eager=True,
        gpu_memory_utilization=0.85,
        num_scheduler_steps=1,
    ),
)

backend = LocalBackend(in_process=True, path="./.art")

# ================================
# STEP 9: TRAINING LOOP
# ================================

def random_sample(samples: List[OpticalSample], n=16) -> List[OpticalSample]:
    """Sample a random batch from training examples"""
    return random.sample(samples, min(n, len(samples)))

async def train():
    """Main training loop using direct execution with model saving"""
    trainer = OpticalRLTrainer("")
    logger.info("üîç Checking for existing training data...")
    generated_trajectories = trainer.get_or_generate_data(
        n_examples=10,
        trajectories_per_example=2,
        force_regenerate=False
    )
    training_samples = [convert_generated_trajectory_to_optical_sample(traj) 
                       for traj in generated_trajectories]
    unique_samples = []
    seen_prompts = set()
    for sample in training_samples:
        if sample.prompt not in seen_prompts:
            unique_samples.append(sample)
            seen_prompts.add(sample.prompt)
    logger.info(f"Created {len(unique_samples)} unique training samples from {len(generated_trajectories)} trajectories")
    
    await model.register(backend)
    
    for step in range(await model.get_step(), 20):
        logger.info(f"Starting training step {step}")
        train_groups = await art.gather_trajectory_groups(
            (
                art.TrajectoryGroup(
                    rollout_with_direct_execution(model, sample) for sample in random_sample(unique_samples, 16)
                )
                for _ in range(1)
            ),
            pbar_desc=f"Gathering Step {step}",
        )
        await model.delete_checkpoints()
        await model.train(
            train_groups,
            config=art.TrainConfig(
                learning_rate=3e-5,
                batch_size=4,
            ),
            _config={"logprob_calculation_chunk_size": 4},
        )
        logger.info(f"Completed training step {step}")
    print("Training Completed")

# ================================
# STEP 10: MAIN EXECUTION
# ================================

if __name__ == "__main__":
    asyncio.run(train())